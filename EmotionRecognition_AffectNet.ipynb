{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionRecognition_AffectNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPfmnOTrfSzJ75Vks7nRryS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElhadjHoussem/Colab/blob/master/EmotionRecognition_AffectNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUpvpbm0u-_F"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8nnQjbbwPDw"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW3uK6nWxgDv"
      },
      "source": [
        "! mkdir ~/.kaggle "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-Dr7_ugxgvL"
      },
      "source": [
        "! cp /content/gdrive/MyDrive/GoogleColab/kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDjpEvWCxts6"
      },
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuJgPTo6xyX5"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkQQLUctx0bc"
      },
      "source": [
        "! kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnQvwCc6x2Wa"
      },
      "source": [
        "! kaggle datasets download -d ptfrwrd/affectnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frs0j4sAx7sG"
      },
      "source": [
        "! mkdir downloads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhgaSdCIyHu_"
      },
      "source": [
        "! unzip affectnet.zip -d downloads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTroAphLyO8t"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image,ImageOps\n",
        "import tqdm\n",
        "import glob\n",
        "#tf.enable_eager_execution()\n",
        "import tensorflow.contrib.eager as tfe\n",
        "########################################################################################################################\n",
        "'''Global Variables'''\n",
        "########################################################################################################################\n",
        "\n",
        "LABELS=['Neutral','Happy','Sad','Surprise','Fear','Disgust','Anger','Contempt','None','Uncertain','Non_Face']\n",
        "NUM_CLASSES = len(LABELS)\n",
        "\n",
        "ZIP_FILE_NAME = \"J:/Emotion/AffectNet.zip\"\n",
        "RECORD_RIR=\"AffectNetRecords_64x64_gray/\"\n",
        "ANNOTATION_SUFFIX_KEYS=['aro','val','exp']\n",
        "ANNOTATION_TYPES={'aro':'float','val' :'float','exp':'int'}\n",
        "DATA_DICT_KEYS=['image','expression','arousal','valence']\n",
        "\n",
        "ANNOTATION_MAP={annotation:key  for annotation in ANNOTATION_SUFFIX_KEYS for key in DATA_DICT_KEYS if annotation in key}\n",
        "\n",
        "IMAGE_SIZE=64\n",
        "COLORS=['RGB','GRAY']\n",
        "COLOR=COLORS[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obE6FW2IyggV"
      },
      "source": [
        "########################################################################################################################\n",
        "'''Functions for Loading Data From Zip File'''\n",
        "########################################################################################################################\n",
        "\n",
        "def load_annotation_from_zipFile(file_name,annotation_Suffix):\n",
        "    annotations={key:[] for key in annotation_Suffix}\n",
        "    with ZipFile(file_name,'r') as zip_archive:\n",
        "        for file in zip_archive.namelist():\n",
        "            paths = file.split(sep='/')\n",
        "            if paths[1] == 'annotations':\n",
        "                annotation_suffix = paths[-1].split('_')[-1].split('.')[0]\n",
        "                if annotation_suffix in annotation_Suffix:\n",
        "                    annotations[annotation_suffix].append(np.load(zip_archive.open(file)))\n",
        "    return annotations\n",
        "def load_data_point_from_zipFile(file_name,data_dict_keys,annotation_Suffix_keys,annotation_map,annotation_types):\n",
        "    Data_point={key:None for key in data_dict_keys}\n",
        "    Annotations = {key:None for key in annotation_Suffix_keys}\n",
        "    with ZipFile(file_name,'r') as zip_archive:\n",
        "        for file in zip_archive.namelist():\n",
        "            paths = file.split(sep='/')\n",
        "            if paths[1] == 'annotations':\n",
        "                annotation_suffix = paths[-1].split('_')[-1].split('.')[0]\n",
        "                if annotation_suffix in annotation_Suffix_keys:\n",
        "                    Annotations[annotation_suffix]=np.load(zip_archive.open(file))\n",
        "                    Annotation_Loaded = not (None in Annotations.values())\n",
        "                    if Annotation_Loaded:\n",
        "                        for annotation_suffix in annotation_Suffix_keys:\n",
        "                            Annotations[annotation_suffix]=np.array(Annotations[annotation_suffix],dtype=annotation_types[annotation_suffix])\n",
        "                        image_path = paths[0]+'/images/'+paths[-1].split('_')[0]+'.jpg'\n",
        "                        image_file = zip_archive.open(image_path)\n",
        "                        image = Image.open(image_file)\n",
        "                        if COLOR=='GRAY':\n",
        "                            image = ImageOps.grayscale(image)\n",
        "                        Data_point['image'] = np.array(image.resize((IMAGE_SIZE,IMAGE_SIZE)))\n",
        "                        for Annotation_key in annotation_Suffix_keys:\n",
        "                            Data_point[annotation_map[Annotation_key]]=Annotations[Annotation_key]\n",
        "\n",
        "                        yield Data_point\n",
        "                        Data_point={key:None for key in data_dict_keys}\n",
        "                        Annotations = {key:None for key in annotation_Suffix_keys}\n",
        "def load_data_point_from_zipFile_by_chunks(file_name,data_dict_keys,annotation_Suffix_keys,annotation_map,annotation_types,chunck_size=1000):\n",
        "    Data_Points=[]\n",
        "    for data_point in load_data_point_from_zipFile(file_name,data_dict_keys,annotation_Suffix_keys,annotation_map,annotation_types):\n",
        "        Data_Points.append(data_point)\n",
        "        if len(Data_Points)>=chunck_size:\n",
        "            yield Data_Points\n",
        "            Data_Points=[]\n",
        "\n",
        "\n",
        "def count_annotation(file_name=ZIP_FILE_NAME):\n",
        "    with ZipFile(file_name,'r') as zip_archive:\n",
        "        files = zip_archive.namelist()\n",
        "\n",
        "    return int(len(files)/5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGKXNXPDyjkz"
      },
      "source": [
        "########################################################################################################################\n",
        "'''TF_RECORD HELPER Functions'''\n",
        "########################################################################################################################\n",
        "def _bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "    if isinstance(value, type(tf.constant(0))): # if value ist tensor\n",
        "        value = value.numpy() # get value of tensor\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns a floast_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "def serialize_array(array):\n",
        "  array = tf.io.serialize_tensor(array)\n",
        "  return array\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRLo5PqsypWR"
      },
      "source": [
        "\n",
        "########################################################################################################################\n",
        "''' TF_RECORD Feature Mapping Function'''\n",
        "########################################################################################################################\n",
        "\n",
        "'''while writing Record'''\n",
        "def parse_single_image(DataPoint):\n",
        "\n",
        "  #define the dictionary -- the structure -- of a single example\n",
        "  data = {\n",
        "      'raw_image': _bytes_feature(serialize_array(DataPoint['image'])),\n",
        "      'height': _int64_feature(DataPoint['image'].shape[0]),\n",
        "      'width': _int64_feature(DataPoint['image'].shape[1]),\n",
        "      'expression': _int64_feature(int(DataPoint['expression'])),\n",
        "      'arousal': _float_feature(float(DataPoint['arousal'])),\n",
        "      'valence': _float_feature(float(DataPoint['valence']))\n",
        "    }\n",
        "  #create an Example, wrapping the single features\n",
        "  out = tf.train.Example(features=tf.train.Features(feature=data))\n",
        "\n",
        "  return out\n",
        "\n",
        "'''while reading Record'''\n",
        "def parse_tfr_element(element):\n",
        "\n",
        "    data = {\n",
        "        'raw_image' : tf.io.FixedLenFeature([], tf.string),\n",
        "        'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'width':tf.io.FixedLenFeature([], tf.int64),\n",
        "        'expression':tf.io.FixedLenFeature([], tf.int64),\n",
        "        'arousal':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'valence':tf.io.FixedLenFeature([], tf.float32)\n",
        "    }\n",
        "\n",
        "    content = tf.io.parse_single_example(element, data)\n",
        "\n",
        "    raw_image = content['raw_image']\n",
        "    height = content['height']\n",
        "    width = content['width']\n",
        "    expression = content['expression']\n",
        "    arousal = content['arousal']\n",
        "    valence = content['valence']\n",
        "\n",
        "\n",
        "    #get our 'feature'-- our image -- and reshape it appropriately\n",
        "    image = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n",
        "    image = tf.reshape(image, shape=[height,width])\n",
        "\n",
        "    return image, expression,arousal,valence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFF6SeyQytzN"
      },
      "source": [
        "\n",
        "########################################################################################################################\n",
        "''' Read/Write  the TFRecord'''\n",
        "########################################################################################################################\n",
        "\n",
        "'''writer functions'''\n",
        "def write_images_to_tfr_(images, labels, filename:str=\"_images\", max_files:int=10, out_dir:str=\"content/\"):\n",
        "\n",
        "    #determine the number of shards (single TFRecord files) we need:\n",
        "    splits = (len(images)//max_files) + 1 #determine how many tfr shards are needed\n",
        "    if len(images)%max_files == 0:\n",
        "        splits-=1\n",
        "    print(f\"\\nUsing {splits} shard(s) for {len(images)} files, with up to {max_files} samples per shard\")\n",
        "\n",
        "    file_count = 0\n",
        "\n",
        "    for i in tqdm.tqdm(range(splits)):\n",
        "        current_shard_name = \"{}{}_{}{}.tfrecords\".format(out_dir, i+1, splits, filename)\n",
        "        writer = tf.io.TFRecordWriter(current_shard_name)\n",
        "\n",
        "        current_shard_count = 0\n",
        "        while current_shard_count < max_files: #as long as our shard is not full\n",
        "            #get the index of the file that we want to parse now\n",
        "            index = i*max_files+current_shard_count\n",
        "            if index == len(images): #when we have consumed the whole data, preempt generation\n",
        "                break\n",
        "            current_image = images[index]\n",
        "            current_label = labels[index]\n",
        "\n",
        "            #create the required Example representation\n",
        "            out = parse_single_image(image=current_image, label=current_label)\n",
        "\n",
        "            writer.write(out.SerializeToString())\n",
        "            current_shard_count+=1\n",
        "            file_count += 1\n",
        "\n",
        "        writer.close()\n",
        "    print(f\"\\nWrote {file_count} elements to TFRecord\")\n",
        "    return file_count\n",
        "\n",
        "def write_images_to_tfr(tfrecord_filename:str=\"_AffectNet\", chunk_size:int=10, out_dir:str=RECORD_RIR):\n",
        "\n",
        "    total_image = count_annotation(ZIP_FILE_NAME,ANNOTATION_SUFFIX_KEYS)\n",
        "    #determine the number of shards (single TFRecord files) we need:\n",
        "    splits = (total_image//chunk_size) + 1 #determine how many tfr shards are needed\n",
        "    if total_image%chunk_size == 0:\n",
        "        splits-=1\n",
        "    print(f\"\\nUsing {splits} shard(s) for {total_image} files, with up to {chunk_size} samples per shard\")\n",
        "\n",
        "    file_count = 0\n",
        "    data_gen = load_data_point_from_zipFile(ZIP_FILE_NAME,DATA_DICT_KEYS,ANNOTATION_SUFFIX_KEYS,ANNOTATION_MAP,ANNOTATION_TYPES)\n",
        "\n",
        "    for i in tqdm.tqdm(range(splits),desc=\"Global Progress All-Files \"+\" {} -> {}\".format(file_count,total_image)):\n",
        "        current_shard_name = \"{}{}_{}{}.tfrecords\".format(out_dir, i+1, splits, tfrecord_filename)\n",
        "        writer = tf.io.TFRecordWriter(current_shard_name)\n",
        "        current_shard_count = 0\n",
        "\n",
        "        for _ in tqdm.tqdm(range(chunk_size),desc=\"Local Progress File \"+ current_shard_name + \" {} ->{} \".format(current_shard_count,chunk_size)):\n",
        "\n",
        "            current_data = next(data_gen)\n",
        "\n",
        "            out = parse_single_image(DataPoint=current_data)\n",
        "\n",
        "            writer.write(out.SerializeToString())\n",
        "            current_shard_count+=1\n",
        "            file_count += 1\n",
        "\n",
        "        writer.close()\n",
        "    print(f\"\\nWrote {file_count} elements to TFRecord\")\n",
        "    return file_count\n",
        "\n",
        "'''Reader functions'''\n",
        "\n",
        "def get_dataset(batch_size,data_size,tfr_dir:str=RECORD_RIR, pattern:str=\"*_AffectNet.tfrecords\"):\n",
        "\n",
        "    train_size = int(0.7 * data_size)\n",
        "    val_size = int(0.3 * data_size)\n",
        "    file=tf.data.Dataset.list_files(tfr_dir+pattern)\n",
        "\n",
        "    full_dataset = tf.data.TFRecordDataset(file)\n",
        "    full_dataset = full_dataset.shuffle(data_size)\n",
        "\n",
        "\n",
        "\n",
        "    train_dataset = full_dataset.take(train_size)\n",
        "    val_dataset = full_dataset.skip(val_size)\n",
        "\n",
        "    train_dataset = train_dataset.repeat()\n",
        "    val_dataset = val_dataset.repeat()\n",
        "\n",
        "    train_dataset = train_dataset.shuffle(batch_size*50)\n",
        "    val_dataset = val_dataset.shuffle(batch_size*10)\n",
        "\n",
        "\n",
        "    train_dataset = train_dataset.map(parse_tfr_element)\n",
        "    val_dataset = val_dataset.map(parse_tfr_element)\n",
        "\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "    return train_dataset,val_dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhR5kvkVwP5a"
      },
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D,Input\n",
        "from tensorflow.keras.losses import categorical_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Bzt6G7whHh"
      },
      "source": [
        "class Network():\n",
        "    \n",
        "    def __init__(self,num_labels,width,height):\n",
        "        self.num_labels = num_labels\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "    \n",
        "    def creat_graph(self,):\n",
        "\n",
        "        model_input = keras.layers.Input(shape=(self.width,self.height,1))\n",
        "\n",
        "        Conv_Layer_1 = keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')(model_input)\n",
        "        Conv_Layer_2 = keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')(Conv_Layer_1)\n",
        "        Pool_Layer_1 = MaxPooling2D(pool_size=(2,2), strides=(2, 2))(Conv_Layer_2)\n",
        "\n",
        "        Conv_Layer_3 = keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')(Pool_Layer_1)\n",
        "        Drop_Layer_1 = keras.layers.Dropout(0.4)(Conv_Layer_3)\n",
        "        Conv_Layer_4 = keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu')(Drop_Layer_1)\n",
        "\n",
        "        Batch_Norm_1 = keras.layers.BatchNormalization()(Conv_Layer_4)\n",
        "        Conv_Layer_5 = keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu')(Batch_Norm_1)\n",
        "        Batch_Norm_2 = keras.layers.BatchNormalization()(Conv_Layer_5)\n",
        "\n",
        "        Conv_Layer_6 = keras.layers.Conv2D(256, kernel_size=(3, 3), activation='relu')(Batch_Norm_2)\n",
        "        Drop_Layer_2 = keras.layers.Dropout(0.4)(Conv_Layer_6)\n",
        "        Conv_Layer_7 = keras.layers.Conv2D(256, kernel_size=(3, 3), activation='relu')(Drop_Layer_2)\n",
        "        Pool_Layer_2 = MaxPooling2D(pool_size=(2,2), strides=(2, 2))(Conv_Layer_7)\n",
        "        Conv_Layer_8 = keras.layers.Conv2D(256, kernel_size=(3, 3), activation='relu')(Pool_Layer_2)\n",
        "\n",
        "        Flat_Layer = Flatten()(Conv_Layer_8)\n",
        "\n",
        "        Dense_layer_1 = keras.layers.Dense(1024, activation='relu')(Flat_Layer)\n",
        "        Output_Layer = keras.layers.Dense(self.num_labels, activation='softmax')(Dense_layer_1)\n",
        "\n",
        "        #Create your model\n",
        "        self.model = keras.models.Model(inputs=model_input, outputs=Output_Layer)\n",
        "        self.model.summary()\n",
        "\n",
        "        #Compile your model\n",
        "        self.model.compile( optimizer=keras.optimizers.RMSprop(lr=0.0003),\n",
        "                            loss=categorical_crossentropy,\n",
        "                            metrics=['accuracy'])\n",
        "    def Train(self,epochs,batch_size,count_data):\n",
        "        #Training the model\n",
        "        Train_data,Val_data = get_dataset(batch_size,count_data,tfr_dir=RECORD_RIR,pattern=PATTERN)\n",
        "\n",
        "        train_iterator = Train_data.make_one_shot_iterator()\n",
        "        val_iterator = Val_data.make_one_shot_iterator()\n",
        "\n",
        "        self.train_images, self.train_labels,_,_ = train_iterator.get_next()\n",
        "        self.val_images, self.val_labels,_,_ = val_iterator.get_next()\n",
        "\n",
        "        # set the pictures to the the proper dimentions\n",
        "        self.train_input = tf.reshape(self.train_images, [-1, self.width,self.height, 1])\n",
        "        self.val_input = tf.reshape(self.val_images, [-1, self.width,self.height, 1])\n",
        "\n",
        "        # Create a one hot array for the labels\n",
        "        self.train_labels = tf.one_hot(self.train_labels, self.num_labels)\n",
        "        self.val_labels = tf.one_hot(self.val_labels, self.num_labels)\n",
        "\n",
        "        with tf.device(\"/gpu:0\"):\n",
        "            self.model.fit(x=self.train_input,y=self.train_labels,validation_data=(self.val_input,self.val_labels),epochs=epochs,steps_per_epoch =int( count_data/batch_size))\n",
        "  \n",
        "    def SaveModel(self,path):\n",
        "        fer_json = self.model.to_json()\n",
        "        with open(path+\"fer.json\", \"w\") as json_file:\n",
        "            json_file.write(fer_json)\n",
        "            self.model.save_weights(\"fer.h5\")\n",
        "\n",
        "    def predict(self,image):\n",
        "        #Training the model\n",
        "        with tf.device(\"/gpu:0\"):\n",
        "            return self.model.predict(image)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZSWI9OOwtf7"
      },
      "source": [
        "RECORD_RIR=\"AffectNetRecords_64x64_gray/\"\n",
        "LABELS=['Neutral','Happy','Sad','Surprise','Fear','Disgust','Anger','Contempt','None','Uncertain','Non_Face']\n",
        "NUM_CLASSES = len(LABELS)\n",
        "count_data=count_annotation()\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "width, height = 64, 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4ZD2dB4wx4s"
      },
      "source": [
        "model = Network(num_labels=NUM_CLASSES,width=width,height=height)\n",
        "model.creat_graph()\n",
        "model.Train(epochs=epochs,batch_size=batch_size,count_data=count_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}